---
title: "Bank Marketing Case"
date: "2/5/2022"
output: html_document
---

```{r setup, include=FALSE}
library(readr)
library(gmodels)
library(tidyverse)
library(car)
library(olsrr)
library(DescTools)
library(ResourceSelection)
library(performance)
library(ROCR)
library(lmtest)
library(broom)
library(ROSE)
knitr::opts_chunk$set(echo = TRUE)
```


## Load the file 

```{r Load the file}
bank <- read_delim("bank-additional.csv", 
                              delim = ";", escape_double = FALSE, trim_ws = TRUE)
```

<br/><br/>

## Overview of the dataset

```{r Overview of the dataset}
str(bank)
```

<br/><br/>

## Data Cleaning and Preparation

### 1) Character Variables to Factor Variables


```{r Data Cleaning: character to factor}

bank$job = as.factor(bank$job)
bank$education = as.factor(bank$education)
bank$marital = as.factor(bank$marital)
bank$default = as.factor(bank$default)
bank$housing = as.factor(bank$housing)
bank$loan = as.factor(bank$loan)
bank$contact = as.factor(bank$contact)
bank$month = as.factor(bank$month)
bank$day_of_week = as.factor(bank$day_of_week)
bank$poutcome = as.factor(bank$poutcome)
bank$y = as.factor(bank$y)

```

<br/><br/>

### 2) Summary: Looking for odd patterns


```{r Overview: Summary}
summary(bank)
```

<br/><br/>

### 3) Finding Duplicates


```{r Finding duplicates}

sum(duplicated(bank))

```
There are no duplicates in the dataset.

<br/><br/>

### 4) Verify numeric variables with NAs

```{r Data Cleaning: Verify numeric variables with NAs}

sum(is.na(bank$age))
sum(is.na(bank$campaign))
sum(is.na(bank$pdays))
sum(is.na(bank$previous))
sum(is.na(bank$emp.var.rate))
sum(is.na(bank$cons.price.idx))
sum(is.na(bank$cons.conf.idx))
sum(is.na(bank$nr.employed))

```

There are no missing values found. 

<br/><br/>


### 5) Exploratory data analysis

#### Job variable

```{r job barplot}
ggplot(bank,aes(job))+geom_bar(aes(fill= y),position = position_dodge())+
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
```

**Analysis:** Admins are the largest group who subscribed to the term deposit and this might be due admins are the largest group in the distribution. 



#### Marital variable

```{r marital bar plot}
ggplot(bank,aes(marital))+geom_bar(aes(fill= y),position = position_dodge())+ theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
```

**Analysis:** Keep variable


#### Education

```{r education barplot}
ggplot(bank,aes(education))+geom_bar(aes(fill= y), position = position_dodge())+ theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
```

**Analysis:** Keep variable


#### Age Boxplot

```{r age boxplot}

ggplot(bank,aes(x = y,y = age))+geom_boxplot(aes(fill= y))+xlab("Subscribed")

```

**Analysis:** We can observe that age is not so good predictor for the whether customer subscribed term deposit, since the mean is almost same. We recommend NOT TO KEEP IT in the model. 

#### Day_of_week Barplot

```{r day_of_week barplot}
ggplot(bank,aes(day_of_week))+geom_bar(aes(fill= y), position = position_dodge())+ theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
```

**Analysis:** We observe that the number of customers subscribed are fairly equal among all the days of the week.Hence this might not be a good predictor for our response variable.


#### Month Barplot

```{r month barplot}
ggplot(bank,aes(month))+geom_bar(aes(fill= y), position = position_dodge())+ theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
```

**Analysis:** From the plots, we observe that May has highest number of subscribers and this would be a good predictor


#### Housing Barplot

```{r housing bar plot}
ggplot(bank,aes(housing))+geom_bar(aes(fill= y), position = position_dodge())+ theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
```
**Analysis:** From the plots, we observe that there is no significance of housing on the number of subscribers

### 6) Analyzing variables with "unknown" categories

#### Cross Tables

```{r Overview of the data - Job}

CrossTable(bank$job, bank$y)

```

**Analysis:** We will be using a 95% critical value, with a df = 11 (12 levels with the unknown - 1 per formula). Critical Value = 4.575 (from Chi-square table). With the cross-table we actually saw that most of the levels don't get passed the critical value, which means that they are significant. 


**Recommendation to remove "unknown":** Even when the chi-square contribution of "unknown" doesn't get passed the critical value, which makes it significant, this is a variable that has only 39 observations, only 4 "yes" out of 451 of the variable, so we recommend to remove them. 


<br/><br/>

#### Cross table - Marital

```{r Overview of the data - Marital}

CrossTable(bank$marital, bank$y)

```

**Analysis and recommendation:** Same as "job" , the "unknown" category for "marital" has very few observations (10 = no, and 1 = yes), we recommend to remove it. 


#### Cross-table for Education

```{r Overview of the data - Education}

CrossTable(bank$education, bank$y)

```

#### Chi-square test for Education

```{r chi-square for Education, warning=FALSE}

chisq.test(bank$education, bank$y)

```

**Analysis and recommendation:** The variable "Education" is significant (basing our decision on Ho: m1 = m2 = m3 = m4 ..., Ha: at least one of the groups has a different mean), removing the "unknown" might be a good option, even when the chi-square contribution is as high as other categories. 


```{r Overview of the data - Housing}

CrossTable(bank$housing, bank$y)

```

#### Chi-square test: Housing

```{r chi-square for Housing}

chisq.test(bank$housing, bank$y)

```

**Analysis and recommendation:** The variable "Housing" is not significant (basing our decision on Ho: m1 = m2 = m3 = m4 ..., Ha: at least one of the groups has a different mean). We recommend NOT TO USE it in the model. 


#### Cross-table for Personal Loan

```{r Overview of the data - Personal Loan}

CrossTable(bank$loan, bank$y)

```

#### Chi-square test: Personal Loan

```{r chi-square for Personal Loan}

chisq.test(bank$loan, bank$y)

```

**Analysis and recommendation:** The variable "Loan" is not significant (basing our decision on Ho: m1 = m2 = m3 = m4 ..., Ha: at least one of the groups has a different mean). We recommend NOT TO USE it in the model. 


### 7) Cleaning data: Modifying pdays 

```{r distribution of pdays}
table(bank$pdays)
```

We saw that we had 3959 observations with the value of "999", that means not ever contacted before, so we decided to recode the variable as a binary variable, where "999" = 0 (not contacted before), and all the other observations would be coded = 1 (contacted).


Just to verify that we did our recoding correctly, and see the final distribution.

```{r Create pdays new variable}

bank$pdays_1<-ifelse(bank$pdays==999,0,1)

table(bank$pdays_1)

```

#### Defining if pdays_1 is significant 

```{r cross table pdays_1}
CrossTable(bank$pdays_1, bank$y)
```

#### Chi-square pdays_1

```{r chi-square pdays_1}

chisq.test(bank$pdays_1, bank$y)

```

**Analysis:** Here we see a "perfect separation" in pdays similar as "Duration". The reason is exactly the same as the one with "Duration".


### 8) Overview of data

#### Is the data balanced or not

```{r is data set Balanced or not}
plot(bank$y)
```
**Analysis:** There are more number of "no" compared to "yes", so the data clearly is imbalanced

### Overview of Data (numeric variables)

#### Scatter plot (pairs)

```{r Overview of data: Pairs_numeric}

pairs1 = bank[, c("y","age","duration","campaign","previous","previous",
                 "emp.var.rate","cons.price.idx","cons.conf.idx")]

pairs(pairs1)

```

### Overview of Data (Factor variables)

#### Scatter plot (pairs)

```{r Overview of data: Pairs_character}

pairs2 = bank[, c("y","job","marital","education","default","housing","loan",
                 "contact","month","day_of_week","poutcome","pdays_1")]

pairs(pairs2)

```
**Analysis:** None of the variables seem to be correlated from the above plots.

### Data Cleaning: 

#### Remove "Unknown" Categories

First step is to create a copy dataset, to start removing those rows. 

```{r Create Copy dataset}
bank_clean <- bank
```


We will remove the "Unknown" Category from the following Factor Variables: Job, Education, Marital.


```{r Remove Unknown Categories: Job}
bank_clean <- subset(bank_clean, bank_clean$job != "unknown")
```

From 4119 observations 4080 (39 for Job)


```{r Remove Unknown Categories: Education}
bank_clean <- subset(bank_clean, bank_clean$education != "unknown")
```

From 4080 observations to 3926 (167 for Education)


```{r Remove Unknown Categories: Marital}
bank_clean <- subset(bank_clean, bank_clean$marital != "unknown")
```

From 3926 observations to 3915 (11 for Marital)


#### Remove variables that are not important

```{r Remove variables }
bank_clean = subset(bank_clean, select = -c(age, duration, default, day_of_week, housing, loan, pdays))

```


#### Analyze dataset: Is dataset balanced?

```{r Balanced dataset or not}
table(bank$y)
table(bank_clean$y)
```

**Analysis:** We can see from the table that the dataset is not balanced, it is more skewed towards "no"


### 9) Logistic Regression Model with all variables

```{r Initial Logistic Regression Model}

Model_1 = glm(data = bank, formula = y ~ .-pdays, family = binomial)

summary(Model_1)
```

**Analysis:** "Duration" attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. This leads to "perfect separation problem" thus, this input should be discarded if the intention is to have a realistic predictive model.

### 9) Creating training and testing samples

```{r training and testing samples}

set.seed(1)
row.number = sample(1:nrow(bank_clean), 0.8*nrow(bank_clean))
train_bank = bank_clean[row.number,]
test_bank = bank_clean[-row.number,]

```



```{r}
train_bank <- ovun.sample(y ~ ., data = train_bank, method = "over",N =4960 )$data
table(train_bank$y)
```

### 10) Model with Dataset not including "Unknown" observations and variables

```{r Logistic Regression Model - not including unknown}

Model_2 = glm(formula = y ~ ., data = train_bank, family = binomial)

summary(Model_2)
```
```{r}

```

### 11) Checking collinearity 


```{r checking for collinearity}

vif(Model_2)

```

**Analysis:** We will run a stepwise removal of the variables, by doing the AIC 


### Stepwise selection of the variables

```{r Stepwise selection - full and null}

glm.null.train_bank = glm(y ~ 1, data = train_bank, family = "binomial")
glm.full.train_bank = glm(y ~ ., 
                      data = train_bank, family = "binomial")

```



```{r Stepwise selection}


step.AIC1 = step(glm.null.train_bank, scope = list(upper=glm.full.train_bank),
     direction ="both",test ="Chisq", trace = F)


step.AIC1

```


```{r}
summary(step.AIC1)
```

### 13) Goodness of Fit: Hosmer-Lemeshow test 

```{r goodnest of fit}
hoslem.test(step.AIC1$y, fitted(step.AIC1), g=10)

```

**Analysis:** Since the Hosmer-Lemeshow test yielded a large p-value of 0.8341, we do not reject our null hypothesis. This means that the model is adequate


### 13) PredProbs

```{r PredProbs}

test_bank$PredProb = predict.glm(step.AIC1, newdata = test_bank, type = "response")

```


```{r Prediction of subscribe}

test_bank$PredSub = ifelse(test_bank$PredProb >= 0.5, "yes", "no")


table(test_bank$PredSub)
```

```{r Confusion Matrix}

caret::confusionMatrix(test_bank$y,as.factor(test_bank$PredSub))


```

```{r}

PredProb1 = prediction(predict.glm(step.AIC1, newdata = test_bank, type = "response"), test_bank$y)

# Computing threshold for cutoff to best trade off sensitivity and specificity
plot(unlist(performance(PredProb1,'sens')@x.values),unlist(performance(PredProb1,'sens')@y.values), type='l', lwd=2, ylab = "", xlab = 'Cutoff')
mtext('Sensitivity',side=2)
mtext('Sensitivity vs. Specificity Plot for AIC Model', side=3)

# Second specificity in same plot
par(new=TRUE)
plot(unlist(performance(PredProb1,'spec')@x.values),unlist(performance(PredProb1,'spec')@y.values), type='l', lwd=2,col='red', ylab = "", xlab = 'Cutoff')
axis(4,at=seq(0,1,0.2)) 
mtext('Specificity',side=4, col='red')

par(new=TRUE)

min.diff <-which.min(abs(unlist(performance(PredProb1, "sens")@y.values) - unlist(performance(PredProb1, "spec")@y.values)))
min.x<-unlist(performance(PredProb1, "sens")@x.values)[min.diff]
min.y<-unlist(performance(PredProb1, "spec")@y.values)[min.diff]
optimal <-min.x

abline(h = min.y, lty = 3)
abline(v = min.x, lty = 3)
text(min.x,0,paste("optimal threshold=",round(optimal,5)), pos = 4)

```


```{r Prediction of Optimal subscribe}

test_bank$PredSubOptimal = ifelse(test_bank$PredProb >= 0.08, "yes", "no")


table(test_bank$PredSubOptimal)
```

```{r Confusion Matrix for Optimal cutoff}

caret::confusionMatrix(test_bank$y,as.factor(test_bank$PredSubOptimal))

```


**Analysis:** Even though the accuracy decreases, the optimal cutoff gives us a higher true subscription values. Hence it is important to have the optimal cutoff instead of the arbitrary (0.5) cutoff.




